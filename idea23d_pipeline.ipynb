{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDEApath = '/mnt/chenjh/Idea23D/input/case1'\n",
    "num_img = 1 \n",
    "num_draft = 3\n",
    "max_iters = 5\n",
    "outpath = '/mnt/chenjh/Idea23D/output/case1-0410-llava-34b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-10 01:29:58,584] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from http import HTTPStatus\n",
    "from transformers import AutoProcessor, LlavaNextForConditionalGeneration\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "import requests\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "from OpenGL.GL import *\n",
    "from OpenGL.GLU import *\n",
    "import pywavefront\n",
    "import re\n",
    "import replicate\n",
    "    \n",
    "def log(text):\n",
    "    print(f'\\n[IDEA-2-3D]: {text}')\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "import base64\n",
    "import json\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "class lmm_gpt4v:\n",
    "    def __init__(self, api_key=''):\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        \"\"\"Encode PIL image to base64, converting RGBA images to RGB.\"\"\"\n",
    "        if image.mode == 'RGBA':\n",
    "            image = image.convert('RGB')\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "    def inference(self, question: str, image):\n",
    "        \"\"\"Make an inference request to the GPT-4 Vision API with an image and a question.\"\"\"\n",
    "        base64_image = self.encode_image(image)\n",
    "        \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"gpt-4-vision-preview\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": question\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": 300\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "class lmm_llava_34b():\n",
    "    \n",
    "    def __init__(self, model_path = \"llava-hf/llava-v1.6-34b-hf\", gpuid = 0): \n",
    "        self.gpuid = gpuid\n",
    "        from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "        self.processor = LlavaNextProcessor.from_pretrained(model_path)\n",
    "        self.model = LlavaNextForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True) \n",
    "        self.model.to(f\"cuda:{gpuid}\")\n",
    "        \n",
    "    def inference(self, question: str, images_list):\n",
    "        if type(images_list) == list:\n",
    "            # 拼接图像\n",
    "            image = concatenate_images_with_number_label(images_list)\n",
    "        else:\n",
    "            image = images_list\n",
    "        \n",
    "        # 对图像进行比较\n",
    "        prompt = f\"<|im_start|>system\\nAnswer the questions.<|im_end|><|im_start|>user\\n<image>\\n{question}<|im_end|><|im_start|>assistant\\n\"\n",
    "        inputs = self.processor(prompt, image, return_tensors=\"pt\").to(f\"cuda:{self.gpuid}\")\n",
    "        output = self.model.generate(**inputs, max_new_tokens=1000)\n",
    "        res = self.processor.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        start_index = res.find(\"<|im_start|> assistant\\n\")\n",
    "        if start_index != -1:\n",
    "            content = res[start_index + len(\"<|im_start|> assistant\\n\"):]\n",
    "            # log(content)\n",
    "        return content\n",
    "    \n",
    "    def image_caption(self, image):\n",
    "        image_caption_prompt = 'Describe the details of this image in detail, including the color, pose, lighting, and environment of the target object.'\n",
    "        return self.inference(image_caption_prompt, image)\n",
    "    pass\n",
    "\n",
    "\n",
    "class lmm_llava_7b():\n",
    "    \n",
    "    def __init__(self, model_path = \"llava-hf/llava-v1.6-mistral-7b-hf\", gpuid = 0): \n",
    "        self.gpuid = gpuid\n",
    "        from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "        self.processor = LlavaNextProcessor.from_pretrained(model_path)\n",
    "        self.model = LlavaNextForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True) \n",
    "        self.model.to(f\"cuda:{gpuid}\")\n",
    "        \n",
    "    def inference(self, question: str, images_list):\n",
    "        if type(images_list) == list:\n",
    "            # 拼接图像\n",
    "            image = concatenate_images_with_number_label(images_list)\n",
    "        else:\n",
    "            image = images_list\n",
    "        \n",
    "        # 对图像进行比较\n",
    "        \n",
    "        prompt = f\"[INST] <image>\\n{question} [/INST]\"\n",
    "        inputs = self.processor(prompt, image, return_tensors=\"pt\").to(f\"cuda:{self.gpuid}\")\n",
    "        output = self.model.generate(**inputs, max_new_tokens=1000)\n",
    "\n",
    "        res = self.processor.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        result = re.search(r'\\[/INST\\](.*)', res)\n",
    "        if result:\n",
    "            res = result.group(1)\n",
    "            # log(f'lmm res={res}')\n",
    "    \n",
    "        return res\n",
    "    \n",
    "    def image_caption(self, image):\n",
    "        image_caption_prompt = 'Describe the details of this image in detail, including the color, pose, lighting, and environment of the target object.'\n",
    "        return self.inference(image_caption_prompt, image)\n",
    "    pass\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "class text2img_sdxl():\n",
    "    def __init__(self, sdxl_base_path='stabilityai/stable-diffusion-xl-base-1.0', sdxl_refiner_path='stabilityai/stable-diffusion-xl-refiner-1.0', gpuid=1,variant=\"fp16\"):\n",
    "        self.sdxl_base_path=sdxl_base_path\n",
    "        self.sdxl_refiner_path=sdxl_refiner_path\n",
    "        self.gpuid=gpuid\n",
    "        # load both base & refiner\n",
    "        self.base = DiffusionPipeline.from_pretrained(\n",
    "            sdxl_base_path, \n",
    "            torch_dtype=torch.float32,\n",
    "            # variant=\"fp16\", \n",
    "            use_safetensors=True\n",
    "        )\n",
    "        self.base.to(f\"cuda:{gpuid}\")\n",
    "        self.refiner = DiffusionPipeline.from_pretrained(\n",
    "            sdxl_refiner_path,\n",
    "            text_encoder_2=self.base.text_encoder_2,\n",
    "            vae=self.base.vae,\n",
    "            torch_dtype=torch.float32,\n",
    "            use_safetensors=True,\n",
    "            # variant=\"fp16\",\n",
    "        )\n",
    "        self.refiner.to(f\"cuda:{gpuid}\")\n",
    "\n",
    "    def inference(self, prompt):\n",
    "        # Define how many steps and what % of steps to be run on each experts (80/20) here\n",
    "        n_steps = 40\n",
    "        high_noise_frac = 0.8\n",
    "\n",
    "        # run both experts\n",
    "        image = self.base(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=n_steps,\n",
    "            denoising_end=high_noise_frac,\n",
    "            output_type=\"latent\",\n",
    "        ).images\n",
    "        image = self.refiner(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=n_steps,\n",
    "            denoising_start=high_noise_frac,\n",
    "            image=image,\n",
    "        ).images[0]\n",
    "        \n",
    "        return image\n",
    "\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "class text2img_sdxl_replicate():\n",
    "    def __init__(self, replicate_key='see https://replicate.com/stability-ai/sdxl/api'):\n",
    "        self.replicate_key=replicate_key\n",
    "        \n",
    "\n",
    "    def inference(self, prompt):\n",
    "        import replicate\n",
    "        from PIL import Image\n",
    "        import os\n",
    "\n",
    "        replicate = replicate.Client(api_token=self.replicate_key)\n",
    "\n",
    "        input = {\n",
    "            \"width\": 1024,\n",
    "            \"height\": 1024,\n",
    "            \"prompt\": prompt,\n",
    "            \"refine\": \"expert_ensemble_refiner\",\n",
    "            \"apply_watermark\": False,\n",
    "            \"num_inference_steps\": 25\n",
    "        }\n",
    "\n",
    "        output = replicate.run(\n",
    "            \"stability-ai/sdxl:39ed52f2a78e934b3ba6e2a89f5b1c712de7dfea535525255b1aa35c5565e08b\",\n",
    "            input=input\n",
    "        )\n",
    "\n",
    "        response = requests.get(output[0])\n",
    "        image_data = BytesIO(response.content)\n",
    "        image = Image.open(image_data)\n",
    "        \n",
    "        return image\n",
    "\n",
    "    \n",
    "    pass\n",
    "\n",
    "import sys\n",
    "# 添加 tool 目录到 sys.path，使得 Python 能找到你的模块\n",
    "module_path = './tool'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from i23d.TripoSR.run import TripoSRmain\n",
    "\n",
    "class img23d_TripoSR():\n",
    "    \n",
    "    def __init__(self, model_path = 'stabilityai/TripoSR', gpuid=1):\n",
    "        self.gpuid = gpuid\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        \n",
    "    def inference(self, png_path, output_path):\n",
    "        # CUDA_VISIBLE_DEVICES=6 python /mnt/chenjh/Idea23D/tool/i23d/TripoSR/run.py /mnt/chenjh/Idea23D/input/a111.png --output-dir /mnt/chenjh/Idea23D/input --render\n",
    "        print('png_path,=',png_path)\n",
    "        res = TripoSRmain(self.gpuid, self.model_path, png_path, output_path)\n",
    "        # os.system(f'CUDA_VISIBLE_DEVICES={self.gpuid} python {self.model_path}/run.py {png_path} --output-dir {output_path} --render')\n",
    "        return f'{output_path}/mesh.obj'\n",
    "\n",
    "    pass\n",
    "\n",
    "def readimage(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        resized_image = image.resize((256, 256))\n",
    "    return resized_image\n",
    "\n",
    "def writeimage(image, path):\n",
    "    # Check if the directory exists, and create it if it doesn't\n",
    "    directory = os.path.dirname(path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # Save the image to the path\n",
    "    with open(path, 'wb') as file:\n",
    "        image.save(file, 'PNG')  # Use 'PNG' to ensure proper saving of PNG files\n",
    "\n",
    "    \n",
    "class Memory():\n",
    "    # 记忆模块\n",
    "    # 初始idea\n",
    "    idea_input_imglist = []\n",
    "    idea_input_img = None # 合并后的img\n",
    "    idea_input_prompt = ''\n",
    "    \n",
    "    best_img = None\n",
    "    best_prompt = None\n",
    "    best_3d_path = None\n",
    "    \n",
    "    feedback = ''\n",
    "    pass\n",
    "\n",
    "\n",
    "class Iter():\n",
    "    # 每一轮的结果\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "    idea_input_imglist = []\n",
    "    prompt = []\n",
    "    draft_img = []\n",
    "    draft_3d_path = []\n",
    "    best_img = None\n",
    "    best_3d_path = ''\n",
    "    best_prompt = ''\n",
    "    \n",
    "    def clear(self):\n",
    "        self.idea_input_imglist = []\n",
    "        self.prompt = []\n",
    "        self.draft_img = []\n",
    "        self.draft_3d_path = []\n",
    "        self.best_img = None\n",
    "        self.best_3d_path = ''\n",
    "        self.best_prompt = ''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[IDEA-2-3D]: loading lmm...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888d5683d80a46fd992a2ffd5368275c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[IDEA-2-3D]: loading t2i...\n",
      "\n",
      "[IDEA-2-3D]: loading i23d...\n",
      "\n",
      "[IDEA-2-3D]: loading finish.\n"
     ]
    }
   ],
   "source": [
    "# 初始化LMM,T2I,I23D\n",
    "log('loading lmm...')\n",
    "\n",
    "# lmm = lmm_gpt4v('sk-your open ai key')\n",
    "lmm = lmm_llava_34b(model_path = \"/mnt/chenjh/LargeModels/llava-v1.6-34b-hf\", gpuid = 4)\n",
    "# lmm = lmm_llava_7b(model_path = \"/mnt/chenjh/LargeModels/llava-v1.6-mistral-7b-hf\", gpuid = 2)\n",
    "\n",
    "log('loading t2i...')\n",
    "# t2i = text2img_sdxl_replicate(replicate_key='r8_ZCtKyMJqjyqVF76N6mycGNHEgF6cTTF1EZtmG')\n",
    "t2i = text2img_sdxl(sdxl_base_path='/mnt/chenjh/LargeModels/stable-diffusion-xl-base-1.0', \n",
    "                    sdxl_refiner_path='/mnt/chenjh/LargeModels/stable-diffusion-xl-refiner-1.0', \n",
    "                    gpuid=6)\n",
    "\n",
    "log('loading i23d...')\n",
    "i23d = img23d_TripoSR(model_path = '/mnt/chenjh/LargeModels/TripoSR' ,gpuid=7)\n",
    "log('loading finish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 01:30:07,014 - INFO - Initializing model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "png_path,= /mnt/chenjh/Idea23D/output/case1-0409-006-34b/draft/iter-1-0-0/draft.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 01:30:15,849 - INFO - Initializing model finished in 8833.93ms.\n",
      "2024-04-10 01:30:15,853 - INFO - Processing image ...\n",
      "2024-04-10 01:30:17,546 - INFO - Processing image finished in 1692.57ms.\n",
      "2024-04-10 01:30:17,547 - INFO - Running model ...\n",
      "2024-04-10 01:30:17,548 - INFO - Running model ...\n",
      "2024-04-10 01:30:18,104 - INFO - Running model finished in 555.52ms.\n",
      "2024-04-10 01:30:18,105 - INFO - Rendering ...\n",
      "2024-04-10 01:30:22,963 - INFO - Rendering finished in 4858.04ms.\n",
      "2024-04-10 01:30:22,965 - INFO - Exporting mesh ...\n",
      "2024-04-10 01:30:24,723 - INFO - Exporting mesh finished in 1758.55ms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/chenjh/Idea23D/output/case1-0409-006-34b/draft/iter-1-0-0/mesh.obj'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i23d.inference('/mnt/chenjh/Idea23D/output/case1-0409-006-34b/draft/iter-1-0-0/draft.png', '/mnt/chenjh/Idea23D/output/case1-0409-006-34b/draft/iter-1-0-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def concatenate_images_with_number_label(images_list, direction=\"h\", output_folder=f'{outpath}/tmp'):\n",
    "    # Check if images_list contains PIL images\n",
    "    if not all(isinstance(image, Image.Image) for image in images_list):\n",
    "        raise ValueError(\"All images in images_list must be PIL images.\")\n",
    "    \n",
    "    # Check direction parameter\n",
    "    if direction not in [\"h\", \"v\"]:\n",
    "        raise ValueError(\"Invalid direction parameter. It must be 'h' for horizontal or 'v' for vertical concatenation.\")\n",
    "    \n",
    "    # Check output folder\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)  # Create the target folder if it doesn't exist\n",
    "    \n",
    "    # Convert PIL images to numpy arrays for processing\n",
    "    np_images = [np.array(image) for image in images_list]\n",
    "    \n",
    "    # Check if we're concatenating horizontally or vertically and create a canvas\n",
    "    if direction == \"h\":\n",
    "        total_width = sum(image.size[0] for image in images_list)\n",
    "        max_height = max(image.size[1] for image in images_list)\n",
    "        concatenated_image = Image.new('RGB', (total_width, max_height))\n",
    "    elif direction == \"v\":\n",
    "        total_height = sum(image.size[1] for image in images_list)\n",
    "        max_width = max(image.size[0] for image in images_list)\n",
    "        concatenated_image = Image.new('RGB', (max_width, total_height))\n",
    "    \n",
    "    # Paste images onto the canvas\n",
    "    x_offset, y_offset = 0, 0\n",
    "    for image in images_list:\n",
    "        concatenated_image.paste(image, (x_offset, y_offset))\n",
    "        if direction == \"h\":\n",
    "            x_offset += image.size[0]\n",
    "        elif direction == \"v\":\n",
    "            y_offset += image.size[1]\n",
    "    \n",
    "    # Save the image\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
    "    output_path = os.path.join(output_folder, f\"concatenated_image-{timestamp}.png\")\n",
    "    \n",
    "    log(f'concatenated_image output_path={output_path}')\n",
    "    concatenated_image.save(output_path)\n",
    "    \n",
    "    return concatenated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[IDEA-2-3D]: img_list.append(image)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[IDEA-2-3D]: img_list.append(image)\n",
      "\n",
      "[IDEA-2-3D]: img_list size = {len(img_list)}\n",
      "\n",
      "[IDEA-2-3D]: memory.idea_input_imglist size = 2\n",
      "\n",
      "[IDEA-2-3D]: memory.idea_input_imglist = [<PIL.Image.Image image mode=RGB size=256x256 at 0x7FCA891DB0A0>, <PIL.Image.Image image mode=RGB size=256x256 at 0x7FCA56DDD660>]\n",
      "\n",
      "[IDEA-2-3D]: concatenated_image output_path=/mnt/chenjh/Idea23D/output/case1-0409-006-34b/tmp/concatenated_image-20240410013037516787.png\n",
      "\n",
      "[IDEA-2-3D]: memory.idea_input_img = <PIL.Image.Image image mode=RGB size=512x256 at 0x7FCA6D117F40>\n",
      "\n",
      "[IDEA-2-3D]: memory.idea_input_prompt = This brown rabbit [ The image is a close-up photograph of a rabbit. The rabbit has a fluffy, light brown coat with darker brown markings on its face, ears, and body. Its fur appears soft and well-groomed. The rabbit's eyes are open and alert, and it has a small, rounded nose. Its ears are erect and pointed upwards. The rabbit is sitting with its hind legs tucked under its body, and its front paws are slightly extended forward. The background of the image is a solid, dark color, which contrasts with the rabbit's light fur, making the rabbit the central focus of the image. There are no visible texts or logos in the image. The style of the image is realistic, with a clear and detailed depiction of the rabbit. ] uses two front paws to engage in the action of eating this doughnut [ The image presents a single, vibrant pink doughnut, resting on a black background. The doughnut is generously sprinkled with a variety of colorful sprinkles, adding a playful touch to its appearance. The sprinkles are scattered across the entire surface of the doughnut, creating a delightful contrast against the pink icing.].\n",
      "\n",
      "\n",
      "[IDEA-2-3D]: init input prompt = This brown rabbit [ The image is a close-up photograph of a rabbit. The rabbit has a fluffy, light brown coat with darker brown markings on its face, ears, and body. Its fur appears soft and well-groomed. The rabbit's eyes are open and alert, and it has a small, rounded nose. Its ears are erect and pointed upwards. The rabbit is sitting with its hind legs tucked under its body, and its front paws are slightly extended forward. The background of the image is a solid, dark color, which contrasts with the rabbit's light fur, making the rabbit the central focus of the image. There are no visible texts or logos in the image. The style of the image is realistic, with a clear and detailed depiction of the rabbit. ] uses two front paws to engage in the action of eating this doughnut [ The image presents a single, vibrant pink doughnut, resting on a black background. The doughnut is generously sprinkled with a variety of colorful sprinkles, adding a playful touch to its appearance. The sprinkles are scattered across the entire surface of the doughnut, creating a delightful contrast against the pink icing.].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--IDEApath\", type=str, default='/mnt/chenjh/Idea23D/input/case1')\n",
    "# parser.add_argument(\"--num_img\", type=int, default=1, help=\"number of images to generate per prompt\")\n",
    "# parser.add_argument(\"--num_draft\", type=int, default=3, help=\"number of prompts to search each round\")\n",
    "# parser.add_argument(\"--max_iters\", type=int, default=3, help=\"max number of iter rounds\")\n",
    "# parser.add_argument(\"--outpath\", default='/mnt/chenjh/Idea23D/output/case1')\n",
    "# args = parser.parse_args()\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "with open(f'{IDEApath}/idea.txt', 'r') as file:\n",
    "    IdeaContent = file.read()\n",
    "\n",
    "if len(IdeaContent.strip()) == 0:\n",
    "    log('Error: empty Idea.txt')\n",
    "    exit()\n",
    "\n",
    " # 初始化记忆模块\n",
    "memory = Memory()\n",
    "\n",
    "# 将idea中的图，使用LMM生成描述，替换成文字\n",
    "# This brown rabbit <IMG>a1.png</IMG> uses two front paws to engage in the action of eating this doughnut <IMG>a111.png</IMG>.\n",
    "prompt_imagecaption = 'Describe the image in detail.'\n",
    "\n",
    "img_tags = re.findall(r'<IMG>(.*?)<\\/IMG>', IdeaContent)\n",
    "obj_tags = re.findall(r'<OBJ>(.*?)<\\/OBJ>', IdeaContent)\n",
    "\n",
    "img_list = []\n",
    "for img_tag in img_tags:\n",
    "    img_path = f'{IDEApath}/{img_tag}'\n",
    "    image = readimage(img_path)\n",
    "    width, height = image.size\n",
    "    # log('image.size', width, height)\n",
    "    img_list.append(image)\n",
    "    log('img_list.append(image)')\n",
    "    caption = lmm.inference(prompt_imagecaption, image)\n",
    "    IdeaContent = IdeaContent.replace(f'<IMG>{img_tag}</IMG>', f'[{caption}]')\n",
    "    # log(img_tag)\n",
    "\n",
    "    # 将idea中的3d模型，使用blender渲染，生成多视角的图，再使用LMM生成描述，替换成文字\n",
    "for obj_tag in obj_tags:\n",
    "    # TODO...\n",
    "    # img_list.append(image) 将obj渲染的图也融合在一张图里\n",
    "    log(obj_tag)\n",
    "\n",
    "# 更新记忆模块中的idea部分，这部分内容是固定值 不会变的\n",
    "log('img_list size = {len(img_list)}')\n",
    "memory.idea_input_imglist = img_list\n",
    "log(f'memory.idea_input_imglist size = {len(memory.idea_input_imglist)}')\n",
    "log(f'memory.idea_input_imglist = {memory.idea_input_imglist}')\n",
    "memory.idea_input_img = concatenate_images_with_number_label(img_list) # 合并后的img\n",
    "memory.idea_input_prompt = IdeaContent # 最原始的user idea input，obj和png未转换\n",
    "\n",
    "log(f'memory.idea_input_img = {memory.idea_input_img}')\n",
    "\n",
    "log(f'memory.idea_input_prompt = {memory.idea_input_prompt}')\n",
    "\n",
    "log(f'init input prompt = {IdeaContent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[IDEA-2-3D]: iter = 0\n",
      "\n",
      "[IDEA-2-3D]: prompt_gen = Optimize text descriptions based on image content and details to better match user input [User Input]This brown rabbit [ The image is a close-up photograph of a rabbit. The rabbit has a fluffy, light brown coat with darker brown markings on its face, ears, and body. Its fur appears soft and well-groomed. The rabbit's eyes are open and alert, and it has a small, rounded nose. Its ears are erect and pointed upwards. The rabbit is sitting with its hind legs tucked under its body, and its front paws are slightly extended forward. The background of the image is a solid, dark color, which contrasts with the rabbit's light fur, making the rabbit the central focus of the image. There are no visible texts or logos in the image. The style of the image is realistic, with a clear and detailed depiction of the rabbit. ] uses two front paws to engage in the action of eating this doughnut [ The image presents a single, vibrant pink doughnut, resting on a black background. The doughnut is generously sprinkled with a variety of colorful sprinkles, adding a playful touch to its appearance. The sprinkles are scattered across the entire surface of the doughnut, creating a delightful contrast against the pink icing.].\n",
      "[/User Input] and images. Answers are 75 words or less.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[IDEA-2-3D]: new input prompt =  The image shows a brown rabbit sitting on a black background. The rabbit has a fluffy, light brown coat with darker brown markings on its face, ears, and body. Its fur appears soft and well-groomed. The rabbit's eyes are open and alert, and it has a small, rounded nose. Its ears are erect and pointed upwards. The rabbit is sitting with its hind legs tucked under its body, and its front paws are slightly extended forward. The background of the image is a solid, dark color, which contrasts with the rabbit's light fur, making the rabbit the central focus of the image. There are no visible texts or logos in the image. The style of the image is realistic, with a clear and detailed depiction of the rabbit. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 01:30:44,214 - INFO - HTTP Request: POST https://api.replicate.com/v1/predictions \"HTTP/1.1 401 Unauthorized\"\n"
     ]
    },
    {
     "ename": "ReplicateError",
     "evalue": "ReplicateError Details:\ntitle: Unauthenticated\nstatus: 401\ndetail: You did not pass a valid authentication token",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReplicateError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m( num_img): \u001b[38;5;66;03m# 每个prompt生成n张图\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     iters\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mappend(IdeaContent)\n\u001b[0;32m---> 44\u001b[0m     imgtmp \u001b[38;5;241m=\u001b[39m \u001b[43mt2i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIdeaContent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# 图片保存\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     imgpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;250m \u001b[39moutpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/draft/iter-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/draft.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 223\u001b[0m, in \u001b[0;36mtext2img_sdxl_replicate.inference\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    212\u001b[0m replicate \u001b[38;5;241m=\u001b[39m replicate\u001b[38;5;241m.\u001b[39mClient(api_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate_key)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_inference_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m25\u001b[39m\n\u001b[1;32m    221\u001b[0m }\n\u001b[0;32m--> 223\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mreplicate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstability-ai/sdxl:39ed52f2a78e934b3ba6e2a89f5b1c712de7dfea535525255b1aa35c5565e08b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\n\u001b[1;32m    226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    229\u001b[0m image_data \u001b[38;5;241m=\u001b[39m BytesIO(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/mnt/anaconda3/envs/llava/lib/python3.10/site-packages/replicate/client.py:157\u001b[0m, in \u001b[0;36mClient.run\u001b[0;34m(self, ref, input, **params)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     ref: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28minput\u001b[39m: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams: Unpack[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions.CreatePredictionParams\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Any, Iterator[Any]]:  \u001b[38;5;66;03m# noqa: ANN401\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Run a model and wait for its output.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/anaconda3/envs/llava/lib/python3.10/site-packages/replicate/run.py:40\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(client, ref, input, **params)\u001b[0m\n\u001b[1;32m     37\u001b[0m version, owner, name, version_id \u001b[38;5;241m=\u001b[39m identifier\u001b[38;5;241m.\u001b[39m_resolve(ref)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m owner \u001b[38;5;129;01mand\u001b[39;00m name:\n\u001b[1;32m     44\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     45\u001b[0m         model\u001b[38;5;241m=\u001b[39m(owner, name), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m     46\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/anaconda3/envs/llava/lib/python3.10/site-packages/replicate/prediction.py:398\u001b[0m, in \u001b[0;36mPredictions.create\u001b[0;34m(self, version, input, **params)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03mCreate a new prediction for the specified model version.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    393\u001b[0m body \u001b[38;5;241m=\u001b[39m _create_prediction_body(\n\u001b[1;32m    394\u001b[0m     version,\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    397\u001b[0m )\n\u001b[0;32m--> 398\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/predictions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _json_to_prediction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client, resp\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m/mnt/anaconda3/envs/llava/lib/python3.10/site-packages/replicate/client.py:87\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, path, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[1;32m     86\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(method, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 87\u001b[0m     \u001b[43m_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/mnt/anaconda3/envs/llava/lib/python3.10/site-packages/replicate/client.py:367\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_for_status\u001b[39m(resp: httpx\u001b[38;5;241m.\u001b[39mResponse) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[0;32m--> 367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ReplicateError\u001b[38;5;241m.\u001b[39mfrom_response(resp)\n",
      "\u001b[0;31mReplicateError\u001b[0m: ReplicateError Details:\ntitle: Unauthenticated\nstatus: 401\ndetail: You did not pass a valid authentication token"
     ]
    }
   ],
   "source": [
    "iters = Iter(0)\n",
    "\n",
    "for i in range(max_iters):\n",
    "    log(f'iter = {i}')\n",
    "    iters.clear()\n",
    "\n",
    "    for k in range( num_draft):\n",
    "        if i == 0: # initial round\n",
    "            prompt_gen = f'Optimize text descriptions based on image content and details to better match user input [User Input]{memory.idea_input_prompt}[/User Input] and images. Answers are 75 words or less.'\n",
    "            log(f'prompt_gen = {prompt_gen}')\n",
    "            IdeaContent = lmm.inference(prompt_gen, memory.idea_input_img)\n",
    "        else:\n",
    "            # The second round starts with memory+idea input, and the image and best prompt of the best model from the previous round.\n",
    "            prompt_rev = f'Optimize prompt [Prompt]{memory.best_prompt}[/Prompt] based on image content and details to better match user input [User Input]{memory.idea_input_prompt}[/User Input] and images. The first line of the image is the user input. Here\\'s the revision [feedback]{memory.feedback}[/feedback]. Answers are 75 words or less.'\n",
    "            log(f'prompt_rev = {prompt_rev}')\n",
    "            imagetmp = memory.idea_input_img\n",
    "            log(f'imagetmp={imagetmp}')\n",
    "            IdeaContent = lmm.inference(prompt_rev, imagetmp)\n",
    "        # Generate end of prompt, convert to image\n",
    "        log(f'new input prompt = {IdeaContent}')\n",
    "        for j in range( num_img): # Each prompt generates n charts\n",
    "            iters.prompt.append(IdeaContent)\n",
    "            imgtmp = t2i.inference(IdeaContent)\n",
    "            imgpath = f'{ outpath}/draft/iter-{i+1}-{k}-{j}/draft.png'\n",
    "            out3dpath = f'{ outpath}/draft/iter-{i+1}-{k}-{j}'\n",
    "            writeimage(imgtmp, imgpath)\n",
    "            i23d_res = i23d.inference(imgpath, out3dpath)\n",
    "            iters.draft_3d_path.append(i23d_res)\n",
    "            log(f'i23d_res = {i23d_res}')\n",
    "            #  Save 6 rendered images, and then filter, filter out the best prompt into memory.\n",
    "            img_render_list = [readimage(f'{ outpath}/draft/iter-{i+1}-{k}-{j}/render_00{idx}.png') for idx in range(6)]\n",
    "            img_render = concatenate_images_with_number_label(img_render_list) # 6 rendered images merged\n",
    "            iters.draft_img.append(img_render)\n",
    "\n",
    "    # Stitch all the images into one big picture, each row is a draft model, and the best model is filtered together.\n",
    "    append_i = -1\n",
    "    if memory.best_3d_path != None:\n",
    "      iters.prompt.append(memory.best_prompt)\n",
    "      iters.draft_img.append(memory.best_img)\n",
    "      iters.draft_3d_path.append(memory.best_3d_path)\n",
    "      append_i = 0\n",
    "\n",
    "    draft_img_comp = concatenate_images_with_number_label(iters.draft_img, 'v')\n",
    "\n",
    "    # Selection of the best draft model for the current round\n",
    "    prompt_select = f'Each row of these images shows 6 views of a 3D model. Which row of images best meets the user input? [User Input]{memory.idea_input_prompt}[/User Input]. Only return a number in the list {[kj for kj in range( num_draft * num_img)]}, the number of rows. Such as, \\\"1\\\" or \\\"0\\\".'\n",
    "    log(f'prompt_select = {prompt_select}')\n",
    "    best_row = lmm.inference(prompt_select, draft_img_comp)\n",
    "    log(f'best_row answer = {best_row}')\n",
    "    try:\n",
    "        best_row = int(best_row)\n",
    "    except ValueError:\n",
    "        # Handle failure to parse to integers\n",
    "        # Handle errors based on specific needs, such as giving default values or prompting the user to re-enter\n",
    "        if i==0:\n",
    "            best_row = 0\n",
    "        else:\n",
    "            best_row = len(iters.draft_3d_path) - 1\n",
    "        log('Failed to parse best_row as an integer. Using default value.')\n",
    "\n",
    "    log(f'best_row = {best_row}')\n",
    "\n",
    "    k = best_row //  num_img\n",
    "    j = best_row %  num_img\n",
    "    log(f'k={k}, j={j}')\n",
    "\n",
    "    memory.best_prompt = iters.prompt[k *  num_draft + j *  num_img + append_i ]\n",
    "    memory.best_img = iters.draft_img[k *  num_draft + j *  num_img + append_i ]\n",
    "    memory.best_3d_path = iters.draft_3d_path[k *  num_draft + j *  num_img + append_i ]\n",
    "    log(f'memory.best_prompt = {memory.best_prompt}')\n",
    "    log(f'memory.best_img = {memory.best_img}')\n",
    "    log(f'memory.best_3d_path = {memory.best_3d_path}')\n",
    "\n",
    "    # Determine if the output condition is met\n",
    "    # Give feedback\n",
    "    prompt_feedback = f'Does the diagram satisfy the user input? [User Input]{memory.idea_input_prompt}[/User Input]. Returns \"no revision\" if it matches the User Input. Give the correct prompt if it does not.'\n",
    "    log(f'prompt_feedback = {prompt_feedback}')\n",
    "    feedback = lmm.inference(prompt_feedback, memory.best_img)\n",
    "    log(f'feedback answer = {feedback}')\n",
    "    if 'no revision' in feedback:\n",
    "        log('output no revison , finish.')\n",
    "        break\n",
    "    memory.feedback = feedback\n",
    "    pass\n",
    "\n",
    "\n",
    "# End of iteration, save memory best model to outputs\n",
    "log(f'cp {memory.best_3d_path} {outpath}/mesh.obj')\n",
    "os.system(f'cp {memory.best_3d_path} {outpath}/mesh.obj')\n",
    "log(f'finished! check the path {outpath}/mesh.obj')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
